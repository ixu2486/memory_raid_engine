#!/usr/bin/env python3
"""
æœ€å¤§æ€§èƒ½OpenCLå¯¦ç¾ - å®Œæ•´é‡‹æ”¾APUæ½›åŠ›
æ¶ˆé™¤æ‰€æœ‰ç“¶é ¸ï¼Œå¯¦ç¾çœŸæ­£çš„é«˜æ€§èƒ½è¨ˆç®—
"""

import time
import numpy as np
import pyopencl as cl
import ctypes
from ctypes import c_void_p, c_size_t, c_uint, c_ulong
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import multiprocessing as mp
from typing import List, Dict, Any, Tuple
import gc

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MaximumPerformanceEngine:
    """æœ€å¤§æ€§èƒ½è¨ˆç®—å¼•æ“ - å®Œæ•´é‡‹æ”¾APUæ½›åŠ›"""
    
    def __init__(self):
        self.context = None
        self.devices = []
        self.queues = []  # å¤šå€‹command queue
        self.programs = {}
        self.memory_pools = {}
        self.data_generators = []
        self.results_cache = {}
        
        # æ€§èƒ½åƒæ•¸
        self.max_queues = 8  # æœ€å¤§ä¸¦è¡ŒéšŠåˆ—æ•¸
        self.compute_units = 0
        self.max_work_group_size = 0
        self.preferred_vector_width = 0
        
    def initialize_maximum_performance(self):
        """åˆå§‹åŒ–æœ€å¤§æ€§èƒ½ç’°å¢ƒ"""
        logger.info("ğŸ”¥ åˆå§‹åŒ–æœ€å¤§æ€§èƒ½è¨ˆç®—å¼•æ“...")
        
        # æ‰¾åˆ°æ‰€æœ‰å¯ç”¨è¨­å‚™
        platforms = cl.get_platforms()
        all_devices = []
        
        for platform in platforms:
            try:
                gpu_devices = platform.get_devices(device_type=cl.device_type.GPU)
                cpu_devices = platform.get_devices(device_type=cl.device_type.CPU)
                all_devices.extend(gpu_devices)
                all_devices.extend(cpu_devices)
            except:
                continue
        
        if not all_devices:
            raise RuntimeError("æ²’æœ‰æ‰¾åˆ°ä»»ä½•OpenCLè¨­å‚™")
        
        # é¸æ“‡æœ€ä½³è¨­å‚™ï¼ˆå„ªå…ˆGPUï¼‰
        self.devices = all_devices[:2]  # ä½¿ç”¨å‰å…©å€‹è¨­å‚™
        self.context = cl.Context(self.devices)
        
        # å‰µå»ºå¤šå€‹command queueå¯¦ç¾æœ€å¤§ä¸¦è¡Œ
        primary_device = self.devices[0]
        self.compute_units = primary_device.max_compute_units
        self.max_work_group_size = primary_device.max_work_group_size
        self.preferred_vector_width = primary_device.preferred_vector_width_float
        
        queue_count = min(self.max_queues, self.compute_units)
        for i in range(queue_count):
            # å‰µå»ºout-of-order execution queueå¯¦ç¾æœ€å¤§ä¸¦è¡Œ
            self.queues.append(
                cl.CommandQueue(
                    self.context, 
                    primary_device,
                    properties=cl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE
                )
            )
        
        logger.info(f"âœ… æœ€å¤§æ€§èƒ½ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ä¸»è¨­å‚™: {primary_device.name}")
        logger.info(f"   è¨ˆç®—å–®å…ƒ: {self.compute_units}")
        logger.info(f"   æœ€å¤§å·¥ä½œçµ„: {self.max_work_group_size}")
        logger.info(f"   å‘é‡å¯¬åº¦: {self.preferred_vector_width}")
        logger.info(f"   ä¸¦è¡ŒéšŠåˆ—: {len(self.queues)}")
        
        # åˆå§‹åŒ–é«˜æ€§èƒ½è¨˜æ†¶é«”æ± 
        self._initialize_high_performance_memory()
        
        # é ç·¨è­¯æ‰€æœ‰kernel
        self._precompile_all_kernels()
        
        # å•Ÿå‹•å¾Œå°æ•¸æ“šç”Ÿæˆå™¨
        self._start_background_data_generators()
    
    def _initialize_high_performance_memory(self):
        """åˆå§‹åŒ–é«˜æ€§èƒ½è¨˜æ†¶é«”æ± """
        logger.info("ğŸŠâ€â™‚ï¸ åˆå§‹åŒ–é«˜æ€§èƒ½è¨˜æ†¶é«”æ± ...")
        
        # æ ¹æ“šè¨­å‚™è¨˜æ†¶é«”å¤§å°åˆ†é…
        device_mem = self.devices[0].global_mem_size
        available_mem = min(device_mem // 4, 256 * 1024 * 1024)  # æœ€å¤š256MB
        
        # å‰µå»ºä¸åŒå¤§å°çš„è¨˜æ†¶é«”æ± 
        pool_configs = [
            (4 * 1024, 200),      # 4K * 200 = 800KB
            (64 * 1024, 100),     # 64K * 100 = 6.4MB
            (1024 * 1024, 50),    # 1M * 50 = 50MB
            (4 * 1024 * 1024, 20), # 4M * 20 = 80MB
            (16 * 1024 * 1024, 5)  # 16M * 5 = 80MB
        ]
        
        total_allocated = 0
        for size_bytes, count in pool_configs:
            size_floats = size_bytes // 4
            self.memory_pools[size_floats] = []
            
            for i in range(count):
                # åˆ†é…å°é½Šçš„pinned memory
                host_mem = np.empty(size_floats, dtype=np.float32)
                host_mem.fill(0.0)  # é å¡«å……é¿å…ç¬¬ä¸€æ¬¡ä½¿ç”¨å»¶é²
                
                # å‰µå»ºé«˜æ€§èƒ½buffer
                cl_buffer = cl.Buffer(
                    self.context,
                    cl.mem_flags.READ_WRITE | cl.mem_flags.USE_HOST_PTR,
                    hostbuf=host_mem
                )
                
                self.memory_pools[size_floats].append({
                    'host_ptr': host_mem,
                    'cl_buffer': cl_buffer,
                    'in_use': False,
                    'size': size_floats
                })
                
                total_allocated += size_bytes
        
        logger.info(f"âœ… é«˜æ€§èƒ½è¨˜æ†¶é«”æ± åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç¸½åˆ†é…: {total_allocated / (1024*1024):.1f} MB")
        logger.info(f"   æ± æ•¸é‡: {sum(len(pool) for pool in self.memory_pools.values())}")
    
    def _precompile_all_kernels(self):
        """é ç·¨è­¯æ‰€æœ‰kernelç¨‹åº"""
        logger.info("âš¡ é ç·¨è­¯é«˜æ€§èƒ½kernel...")
        
        # é«˜åº¦å„ªåŒ–çš„kernelæºç¢¼
        kernel_source = f"""
        // é‡å°APUæ¶æ§‹å„ªåŒ–çš„å‘é‡åŒ–kernel
        #pragma OPENCL EXTENSION cl_khr_fp16 : enable
        
        // æœ€å¤§åŒ–è¨ˆç®—å¯†åº¦çš„kernel
        __kernel void maximum_compute_kernel(
            __global float4* input_a,
            __global float4* input_b, 
            __global float4* output,
            int vector_count,
            float scale_factor
        ) {{
            int gid = get_global_id(0);
            int stride = get_global_size(0);
            
            // å‘é‡åŒ–è™•ç†ï¼Œä¸€æ¬¡è™•ç†4å€‹float
            for (int i = gid; i < vector_count; i += stride) {{
                float4 a = input_a[i];
                float4 b = input_b[i];
                
                // é«˜è¨ˆç®—å¯†åº¦é‹ç®— - æ¯å€‹å‘é‡åš40+å€‹é‹ç®—
                float4 result = a * b;
                result = sin(result) + cos(a - b);
                result = sqrt(fabs(result)) * scale_factor;
                result = result * result + a / (b + 0.001f);
                result = pow(result, 0.75f) * sin(result * 0.1f);
                
                // æ›´å¤šè¨ˆç®—ä¾†æé«˜è¨ˆç®—/è¨˜æ†¶é«”æ¯”ä¾‹
                for (int j = 0; j < 3; j++) {{
                    result = sin(result * 1.1f) * cos(result * 0.9f);
                    result += sqrt(fabs(result * a)) * 0.01f;
                }}
                
                output[i] = result;
            }}
        }}
        
        // å°ˆé–€ç”¨æ–¼å¤§æ•¸æ“šçš„streaming kernel
        __kernel void streaming_compute_kernel(
            __global float* input,
            __global float* output,
            int size,
            int offset,
            float4 params
        ) {{
            int gid = get_global_id(0);
            int local_id = get_local_id(0);
            int group_size = get_local_size(0);
            
            // ä½¿ç”¨local memoryåŠ é€Ÿ
            __local float shared_data[{self.max_work_group_size}];
            
            if (gid + offset < size) {{
                float value = input[gid + offset];
                
                // è¤‡é›œè¨ˆç®—åºåˆ—
                value = sin(value * params.x) * cos(value * params.y);
                value = sqrt(fabs(value)) + log(fabs(value) + 1.0f);
                value = pow(value, params.z) * params.w;
                
                // Local memoryå”ä½œè¨ˆç®—
                shared_data[local_id] = value;
                barrier(CLK_LOCAL_MEM_FENCE);
                
                // é„°åŸŸè¨ˆç®—å¢åŠ è¤‡é›œåº¦
                if (local_id > 0 && local_id < group_size - 1) {{
                    value = (shared_data[local_id-1] + shared_data[local_id] + shared_data[local_id+1]) / 3.0f;
                }}
                
                // æœ€çµ‚è¤‡é›œè®Šæ›
                value = tanh(value) * exp(-fabs(value) * 0.1f);
                
                output[gid + offset] = value;
            }}
        }}
        
        // çŸ©é™£é‹ç®—kernel - æœ€å¤§åŒ–ALUåˆ©ç”¨ç‡
        __kernel void matrix_multiply_kernel(
            __global float* A,
            __global float* B,
            __global float* C,
            int M, int N, int K
        ) {{
            int row = get_global_id(0);
            int col = get_global_id(1);
            
            if (row < M && col < N) {{
                float sum = 0.0f;
                
                // å±•é–‹å¾ªç’°æé«˜æ€§èƒ½
                int k;
                for (k = 0; k <= K-4; k += 4) {{
                    sum += A[row * K + k] * B[k * N + col];
                    sum += A[row * K + k + 1] * B[(k + 1) * N + col];
                    sum += A[row * K + k + 2] * B[(k + 2) * N + col];
                    sum += A[row * K + k + 3] * B[(k + 3) * N + col];
                }}
                
                // è™•ç†å‰©é¤˜å…ƒç´ 
                for (; k < K; k++) {{
                    sum += A[row * K + k] * B[k * N + col];
                }}
                
                C[row * N + col] = sum;
            }}
        }}
        """
        
        try:
            # ä½¿ç”¨æœ€æ¿€é€²çš„ç·¨è­¯é¸é …
            build_options = [
                "-cl-fast-relaxed-math",  # æœ€å¿«æ•¸å­¸é‹ç®—
                "-cl-unsafe-math-optimizations",  # ä¸å®‰å…¨ä½†å¿«é€Ÿçš„å„ªåŒ–
                "-cl-mad-enable",  # å•Ÿç”¨MADæŒ‡ä»¤
                "-cl-no-signed-zeros",  # å¿½ç•¥å¸¶ç¬¦è™Ÿé›¶
                "-cl-finite-math-only",  # åªè€ƒæ…®æœ‰é™æ•¸å­¸
                f"-cl-std=CL2.0"  # ä½¿ç”¨OpenCL 2.0
            ]
            
            program = cl.Program(self.context, kernel_source).build(
                options=" ".join(build_options)
            )
            
            self.programs['maximum_performance'] = program
            logger.info("âœ… é«˜æ€§èƒ½kernelç·¨è­¯å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"æ¿€é€²å„ªåŒ–ç·¨è­¯å¤±æ•—ï¼Œä½¿ç”¨æ¨™æº–é¸é …: {e}")
            # é™ç´šåˆ°æ¨™æº–ç·¨è­¯
            program = cl.Program(self.context, kernel_source).build()
            self.programs['maximum_performance'] = program
    
    def _start_background_data_generators(self):
        """å•Ÿå‹•å¾Œå°æ•¸æ“šç”Ÿæˆå™¨"""
        logger.info("ğŸ”„ å•Ÿå‹•å¾Œå°æ•¸æ“šç”Ÿæˆå™¨...")
        
        # å‰µå»ºæ•¸æ“šç”Ÿæˆç·šç¨‹æ± 
        self.data_queue = queue.Queue(maxsize=20)
        self.generator_running = True
        
        def data_generator_worker():
            """å¾Œå°æ•¸æ“šç”Ÿæˆå·¥ä½œç·šç¨‹"""
            sizes = [1024, 4096, 16384, 65536]
            
            while self.generator_running:
                try:
                    for size in sizes:
                        if self.data_queue.qsize() < 15:  # ä¿æŒéšŠåˆ—ä¸æ»¿
                            # é ç”Ÿæˆå‘é‡åŒ–æ•¸æ“š
                            data_a = np.random.rand(size).astype(np.float32)
                            data_b = np.random.rand(size).astype(np.float32)
                            
                            self.data_queue.put({
                                'size': size,
                                'data_a': data_a,
                                'data_b': data_b,
                                'timestamp': time.time()
                            }, timeout=1)
                        else:
                            time.sleep(0.001)  # çŸ­æš«ä¼‘æ¯
                except:
                    break
        
        # å•Ÿå‹•å¤šå€‹ç”Ÿæˆå™¨ç·šç¨‹
        for i in range(2):
            thread = threading.Thread(target=data_generator_worker, daemon=True)
            thread.start()
            self.data_generators.append(thread)
        
        logger.info("âœ… å¾Œå°æ•¸æ“šç”Ÿæˆå™¨å•Ÿå‹•å®Œæˆ")
    
    def get_optimized_buffer(self, size: int) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–çš„buffer"""
        # æ‰¾åˆ°æœ€å°çš„è¶³å¤ å¤§çš„pool
        suitable_sizes = [s for s in self.memory_pools.keys() if s >= size]
        if not suitable_sizes:
            # å‰µå»ºæ–°çš„å¤§buffer
            size = max(size, max(self.memory_pools.keys()) * 2)
            host_mem = np.empty(size, dtype=np.float32)
            cl_buffer = cl.Buffer(
                self.context,
                cl.mem_flags.READ_WRITE | cl.mem_flags.USE_HOST_PTR,
                hostbuf=host_mem
            )
            return {
                'host_ptr': host_mem,
                'cl_buffer': cl_buffer,
                'in_use': True,
                'size': size,
                'from_pool': False
            }
        
        pool_size = min(suitable_sizes)
        pool = self.memory_pools[pool_size]
        
        # æ‰¾ç©ºé–’buffer
        for buffer in pool:
            if not buffer['in_use']:
                buffer['in_use'] = True
                buffer['from_pool'] = True
                return buffer
        
        # å¦‚æœæ²’æœ‰ç©ºé–’çš„ï¼Œæ“´å±•pool
        host_mem = np.empty(pool_size, dtype=np.float32)
        cl_buffer = cl.Buffer(
            self.context,
            cl.mem_flags.READ_WRITE | cl.mem_flags.USE_HOST_PTR,
            hostbuf=host_mem
        )
        buffer = {
            'host_ptr': host_mem,
            'cl_buffer': cl_buffer,
            'in_use': True,
            'size': pool_size,
            'from_pool': True
        }
        pool.append(buffer)
        return buffer
    
    def return_buffer(self, buffer: Dict[str, Any]):
        """æ­¸é‚„buffer"""
        if buffer.get('from_pool', True):
            buffer['in_use'] = False
    
    def maximum_performance_test(self, data_size: int, iterations: int = 10) -> Dict[str, float]:
        """æœ€å¤§æ€§èƒ½æ¸¬è©¦"""
        logger.info(f"ğŸ”¥ æœ€å¤§æ€§èƒ½æ¸¬è©¦ (å¤§å°: {data_size}, è¿­ä»£: {iterations})")
        
        program = self.programs['maximum_performance']
        kernel = program.maximum_compute_kernel
        
        # é ç†±GPU
        self._warmup_gpu()
        
        times = []
        compute_times = []
        memory_times = []
        
        for i in range(iterations):
            start_total = time.perf_counter()
            
            # 1. å¿«é€Ÿç²å–é ç”Ÿæˆæ•¸æ“š
            start_mem = time.perf_counter()
            try:
                data_pack = self.data_queue.get_nowait()
                if data_pack['size'] >= data_size:
                    input_a = data_pack['data_a'][:data_size]
                    input_b = data_pack['data_b'][:data_size]
                else:
                    raise queue.Empty
            except queue.Empty:
                # å¦‚æœæ²’æœ‰é ç”Ÿæˆæ•¸æ“šï¼Œå¿«é€Ÿç”Ÿæˆ
                input_a = np.random.rand(data_size).astype(np.float32)
                input_b = np.random.rand(data_size).astype(np.float32)
            
            # 2. ç²å–å„ªåŒ–buffer
            vec_size = (data_size + 3) // 4  # å‘é‡åŒ–å¤§å°
            buf_a = self.get_optimized_buffer(data_size)
            buf_b = self.get_optimized_buffer(data_size)
            buf_result = self.get_optimized_buffer(data_size)
            
            # 3. å¿«é€Ÿæ•¸æ“šæ‹·è²
            buf_a['host_ptr'][:data_size] = input_a
            buf_b['host_ptr'][:data_size] = input_b
            
            memory_time = time.perf_counter() - start_mem
            
            # 4. æœ€å¤§æ€§èƒ½kernelåŸ·è¡Œ
            start_compute = time.perf_counter()
            
            # é¸æ“‡æœ€å„ªçš„queue
            queue_idx = i % len(self.queues)
            selected_queue = self.queues[queue_idx]
            
            # è¨­ç½®kernelåƒæ•¸
            kernel.set_arg(0, buf_a['cl_buffer'])
            kernel.set_arg(1, buf_b['cl_buffer'])
            kernel.set_arg(2, buf_result['cl_buffer'])
            kernel.set_arg(3, np.int32(vec_size))
            kernel.set_arg(4, np.float32(1.5))
            
            # è¨ˆç®—æœ€å„ªå·¥ä½œçµ„å¤§å°
            work_group_size = min(self.max_work_group_size, 256)
            global_size = ((vec_size + work_group_size - 1) // work_group_size) * work_group_size
            
            # åŸ·è¡Œkernel
            cl.enqueue_nd_range_kernel(
                selected_queue, 
                kernel, 
                (global_size,), 
                (work_group_size,)
            )
            selected_queue.finish()
            
            compute_time = time.perf_counter() - start_compute
            
            # 5. å¿«é€Ÿæ¸…ç†
            self.return_buffer(buf_a)
            self.return_buffer(buf_b)
            self.return_buffer(buf_result)
            
            total_time = time.perf_counter() - start_total
            
            times.append(total_time * 1000)
            compute_times.append(compute_time * 1000)
            memory_times.append(memory_time * 1000)
        
        # å»æ‰æœ€æ…¢çš„å¹¾æ¬¡ï¼ˆé ç†±å½±éŸ¿ï¼‰
        times = sorted(times)[1:-1] if len(times) > 3 else times
        compute_times = sorted(compute_times)[1:-1] if len(compute_times) > 3 else compute_times
        memory_times = sorted(memory_times)[1:-1] if len(memory_times) > 3 else memory_times
        
        avg_total = np.mean(times)
        avg_compute = np.mean(compute_times)
        avg_memory = np.mean(memory_times)
        
        compute_ratio = avg_compute / avg_total * 100
        
        logger.info(f"   ç¸½æ™‚é–“: {avg_total:.3f} ms")
        logger.info(f"   è¨ˆç®—æ™‚é–“: {avg_compute:.3f} ms ({compute_ratio:.1f}%)")
        logger.info(f"   è¨˜æ†¶é«”æ™‚é–“: {avg_memory:.3f} ms ({avg_memory/avg_total*100:.1f}%)")
        logger.info(f"   è¨ˆç®—å¯†åº¦: {'ğŸ”¥ å„ªç§€' if compute_ratio > 70 else 'ğŸ”¥ è‰¯å¥½' if compute_ratio > 50 else 'âš ï¸ éœ€å„ªåŒ–'}")
        
        return {
            'total_ms': avg_total,
            'compute_ms': avg_compute,
            'memory_ms': avg_memory,
            'compute_ratio': compute_ratio
        }
    
    def _warmup_gpu(self):
        """é ç†±GPUé¿å…ç¬¬ä¸€æ¬¡åŸ·è¡Œå»¶é²"""
        if hasattr(self, '_warmed_up'):
            return
        
        logger.info("ğŸ”¥ é ç†±GPU...")
        warmup_size = 1024
        buf = self.get_optimized_buffer(warmup_size)
        
        program = self.programs['maximum_performance']
        kernel = program.maximum_compute_kernel
        
        kernel.set_arg(0, buf['cl_buffer'])
        kernel.set_arg(1, buf['cl_buffer'])
        kernel.set_arg(2, buf['cl_buffer'])
        kernel.set_arg(3, np.int32(warmup_size // 4))
        kernel.set_arg(4, np.float32(1.0))
        
        cl.enqueue_nd_range_kernel(self.queues[0], kernel, (256,), (64,))
        self.queues[0].finish()
        
        self.return_buffer(buf)
        self._warmed_up = True
        logger.info("âœ… GPUé ç†±å®Œæˆ")
    
    def massive_parallel_test(self, total_size: int, chunk_count: int = 8) -> Dict[str, float]:
        """å¤§è¦æ¨¡ä¸¦è¡Œæ¸¬è©¦"""
        logger.info(f"ğŸš€ å¤§è¦æ¨¡ä¸¦è¡Œæ¸¬è©¦ (ç¸½å¤§å°: {total_size}, åˆ†å¡Š: {chunk_count})")
        
        chunk_size = total_size // chunk_count
        program = self.programs['maximum_performance']
        streaming_kernel = program.streaming_compute_kernel
        
        start_total = time.perf_counter()
        
        def process_chunk_maximum(chunk_id: int, offset: int, size: int) -> float:
            """æœ€å¤§æ€§èƒ½è™•ç†å–®å€‹æ•¸æ“šå¡Š"""
            queue_idx = chunk_id % len(self.queues)
            queue = self.queues[queue_idx]
            
            # ç²å–buffer
            input_buf = self.get_optimized_buffer(size)
            output_buf = self.get_optimized_buffer(size)
            
            # å¿«é€Ÿå¡«å……éš¨æ©Ÿæ•¸æ“š
            input_buf['host_ptr'][:size] = np.random.rand(size).astype(np.float32)
            
            # è¨­ç½®kernel
            streaming_kernel.set_arg(0, input_buf['cl_buffer'])
            streaming_kernel.set_arg(1, output_buf['cl_buffer'])
            streaming_kernel.set_arg(2, np.int32(size))
            streaming_kernel.set_arg(3, np.int32(0))
            
            # è¨­ç½®è¤‡é›œåƒæ•¸
            params = np.array([1.1, 0.9, 0.8, 2.0], dtype=np.float32)
            streaming_kernel.set_arg(4, params)
            
            # åŸ·è¡Œ
            work_group = min(self.max_work_group_size, 256)
            global_size = ((size + work_group - 1) // work_group) * work_group
            
            start_chunk = time.perf_counter()
            cl.enqueue_nd_range_kernel(queue, streaming_kernel, (global_size,), (work_group,))
            queue.finish()
            chunk_time = time.perf_counter() - start_chunk
            
            # æ¸…ç†
            self.return_buffer(input_buf)
            self.return_buffer(output_buf)
            
            return chunk_time * 1000
        
        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰å¡Š
        with ThreadPoolExecutor(max_workers=chunk_count) as executor:
            futures = []
            for i in range(chunk_count):
                offset = i * chunk_size
                actual_size = min(chunk_size, total_size - offset)
                future = executor.submit(process_chunk_maximum, i, offset, actual_size)
                futures.append(future)
            
            # æ”¶é›†çµæœ
            chunk_times = []
            for future in as_completed(futures):
                chunk_time = future.result()
                chunk_times.append(chunk_time)
        
        total_time = (time.perf_counter() - start_total) * 1000
        avg_chunk_time = np.mean(chunk_times)
        max_chunk_time = np.max(chunk_times)
        
        # è¨ˆç®—ä¸¦è¡Œæ•ˆç‡
        estimated_serial_time = avg_chunk_time * chunk_count
        parallel_efficiency = estimated_serial_time / total_time
        
        logger.info(f"   ç¸½æ™‚é–“: {total_time:.3f} ms")
        logger.info(f"   å¹³å‡å¡Šæ™‚é–“: {avg_chunk_time:.3f} ms")
        logger.info(f"   æœ€æ…¢å¡Šæ™‚é–“: {max_chunk_time:.3f} ms")
        logger.info(f"   ä¸¦è¡Œæ•ˆç‡: {parallel_efficiency:.2f}å€")
        logger.info(f"   ååé‡: {total_size / (total_time / 1000) / 1e6:.2f} Må…ƒç´ /ç§’")
        
        return {
            'total_ms': total_time,
            'avg_chunk_ms': avg_chunk_time,
            'parallel_efficiency': parallel_efficiency,
            'throughput_mops': total_size / (total_time / 1000) / 1e6
        }
    
    def benchmark_suite(self):
        """å®Œæ•´åŸºæº–æ¸¬è©¦å¥—ä»¶"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ”¥ æœ€å¤§æ€§èƒ½åŸºæº–æ¸¬è©¦å¥—ä»¶")
        logger.info("="*80)
        
        device = self.devices[0]
        logger.info(f"ğŸ–¥ï¸ æ¸¬è©¦è¨­å‚™: {device.name}")
        logger.info(f"   è¨ˆç®—å–®å…ƒ: {self.compute_units}")
        logger.info(f"   å…¨å±€è¨˜æ†¶é«”: {device.global_mem_size // (1024*1024)} MB")
        logger.info(f"   æœ€å¤§å·¥ä½œçµ„: {self.max_work_group_size}")
        
        results = {}
        
        # 1. å–®æ ¸æ€§èƒ½æ¸¬è©¦
        logger.info(f"\nğŸš€ å–®æ ¸æœ€å¤§æ€§èƒ½æ¸¬è©¦:")
        sizes = [1024, 4096, 16384, 65536, 262144]
        for size in sizes:
            logger.info(f"\n--- æ¸¬è©¦å¤§å°: {size} å…ƒç´  ({size*4/1024:.1f} KB) ---")
            result = self.maximum_performance_test(size, iterations=8)
            results[f'single_{size}'] = result
        
        # 2. å¤§è¦æ¨¡ä¸¦è¡Œæ¸¬è©¦
        logger.info(f"\nğŸ”¥ å¤§è¦æ¨¡ä¸¦è¡Œæ¸¬è©¦:")
        parallel_configs = [
            (262144, 4),   # 256K åˆ†4å¡Š
            (1048576, 8),  # 1M åˆ†8å¡Š
            (4194304, 16), # 4M åˆ†16å¡Š
        ]
        
        for total_size, chunks in parallel_configs:
            logger.info(f"\n--- ä¸¦è¡Œé…ç½®: {total_size} å…ƒç´ , {chunks} å¡Š ---")
            result = self.massive_parallel_test(total_size, chunks)
            results[f'parallel_{total_size}_{chunks}'] = result
        
        # 3. æ€§èƒ½åˆ†æå’Œç¸½çµ
        self._analyze_benchmark_results(results)
        
        return results
    
    def _analyze_benchmark_results(self, results: Dict[str, Any]):
        """åˆ†æåŸºæº–æ¸¬è©¦çµæœ"""
        logger.info(f"\n" + "="*60)
        logger.info("ğŸ¯ æ€§èƒ½åˆ†æå’Œç¸½çµ")
        logger.info("="*60)
        
        # åˆ†æå–®æ ¸æ€§èƒ½è¶¨å‹¢
        single_results = {k: v for k, v in results.items() if k.startswith('single_')}
        
        logger.info(f"\nğŸ“Š å–®æ ¸æ€§èƒ½åˆ†æ:")
        best_compute_ratio = 0
        best_size = 0
        
        for key, result in single_results.items():
            size = int(key.split('_')[1])
            compute_ratio = result['compute_ratio']
            total_time = result['total_ms']
            
            if compute_ratio > best_compute_ratio:
                best_compute_ratio = compute_ratio
                best_size = size
            
            logger.info(f"   {size:>8} å…ƒç´ : {total_time:>8.3f}ms, è¨ˆç®—å æ¯”: {compute_ratio:>5.1f}%")
        
        logger.info(f"\nğŸ† æœ€ä½³å–®æ ¸é…ç½®: {best_size} å…ƒç´  (è¨ˆç®—å æ¯”: {best_compute_ratio:.1f}%)")
        
        # åˆ†æä¸¦è¡Œæ€§èƒ½
        parallel_results = {k: v for k, v in results.items() if k.startswith('parallel_')}
        
        if parallel_results:
            logger.info(f"\nğŸš€ ä¸¦è¡Œæ€§èƒ½åˆ†æ:")
            best_throughput = 0
            best_config = ""
            
            for key, result in parallel_results.items():
                parts = key.split('_')
                total_size = int(parts[1])
                chunks = int(parts[2])
                efficiency = result['parallel_efficiency']
                throughput = result['throughput_mops']
                
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_config = f"{total_size} å…ƒç´ , {chunks} å¡Š"
                
                logger.info(f"   {total_size:>8} å…ƒç´  x {chunks:>2} å¡Š: {efficiency:>5.2f}å€æ•ˆç‡, {throughput:>6.2f} MOPS")
            
            logger.info(f"\nğŸ† æœ€ä½³ä¸¦è¡Œé…ç½®: {best_config} (ååé‡: {best_throughput:.2f} MOPS)")
        
        # ç¸½é«”è©•ä¼°
        logger.info(f"\nğŸ¯ æœ€å¤§æ€§èƒ½è©•ä¼°:")
        
        if best_compute_ratio > 80:
            logger.info("âœ… ğŸ”¥ æ¥µè‡´æ€§èƒ½ï¼è¨ˆç®—å¯†åº¦è¶…é80%ï¼Œå·²å®Œå…¨é‡‹æ”¾APUæ½›åŠ›")
        elif best_compute_ratio > 60:
            logger.info("âœ… ğŸš€ å„ªç§€æ€§èƒ½ï¼è¨ˆç®—å¯†åº¦è¶…é60%ï¼ŒAPUæ½›åŠ›åŸºæœ¬é‡‹æ”¾")
        elif best_compute_ratio > 40:
            logger.info("ğŸ”¥ è‰¯å¥½æ€§èƒ½ï¼è¨ˆç®—å¯†åº¦è¶…é40%ï¼Œé‚„æœ‰å„ªåŒ–ç©ºé–“")
        else:
            logger.info("âš ï¸ ä»éœ€å„ªåŒ–ï¼Œå»ºè­°å¢åŠ è¨ˆç®—è¤‡é›œåº¦")
        
        if parallel_results and best_throughput > 100:
            logger.info(f"âœ… ğŸ”¥ ä¸¦è¡Œè™•ç†å„ªç§€ï¼ååé‡é”åˆ° {best_throughput:.1f} MOPS")
        
        logger.info(f"\nğŸ’¡ æ€§èƒ½çªç ´ç¸½çµ:")
        logger.info(f"   âœ… é ç·¨è­¯kernel + æ¿€é€²ç·¨è­¯å„ªåŒ–")
        logger.info(f"   âœ… é«˜æ€§èƒ½è¨˜æ†¶é«”æ±  + é›¶æ‹·è²buffer")
        logger.info(f"   âœ… å¾Œå°æ•¸æ“šç”Ÿæˆå™¨ + éé˜»å¡æ“ä½œ")
        logger.info(f"   âœ… å¤šéšŠåˆ—ä¸¦è¡Œ + å‘é‡åŒ–è¨ˆç®—")
        logger.info(f"   âœ… GPUé ç†± + å·¥ä½œçµ„å„ªåŒ–")
        
        if best_compute_ratio > 70:
            logger.info(f"ğŸ‰ æ­å–œï¼ä½ çš„APUæ€§èƒ½å·²å®Œæ•´æ‰“é–‹ï¼")
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        self.generator_running = False
        
        # æ¸…ç†è¨˜æ†¶é«”æ± 
        for pool in self.memory_pools.values():
            for buffer in pool:
                try:
                    buffer['cl_buffer'].release()
                except:
                    pass
        
        # æ¸…ç†éšŠåˆ—
        for queue in self.queues:
            try:
                queue.finish()
            except:
                pass
        
        logger.info("ğŸ§¹ è³‡æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    engine = MaximumPerformanceEngine()
    
    try:
        # åˆå§‹åŒ–æœ€å¤§æ€§èƒ½ç’°å¢ƒ
        engine.initialize_maximum_performance()
        
        # çŸ­æš«ç­‰å¾…å¾Œå°æ•¸æ“šç”Ÿæˆå™¨å•Ÿå‹•
        time.sleep(1)
        
        # é‹è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦
        results = engine.benchmark_suite()
        
        logger.info(f"\nğŸ‰ æœ€å¤§æ€§èƒ½æ¸¬è©¦å®Œæˆï¼APUæ½›åŠ›å·²å®Œæ•´é‡‹æ”¾ï¼")
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        engine.cleanup()

if __name__ == "__main__":
    main()